- hosts: servers
  become: yes
  collections:
    - community.docker

  vars:
    users:
      - username: shoeb
        password: "$6$H5OIC/dYaWs9CeIm$Z9C1P8/3iHu2v7Ent2zGiuM2LRxN4otIcx1q7HwuTgAjzmbsBPN5kjuNjoU1SWCBjzPbhttkhNgki8.EAQA4E1"
        email: syedshoeb8380@gmail.com
      - username: abrar
        password: "$6$AJxuHT.47WKkC6af$9MelO.iWFz8RZLBXz9gYcFWi0v.qGxN/tG1uFnVg5p1mm0jdhgfqEclyZ8N34MHTEEa5E92o/zx1E05VVf8ut1"
        email: syedabrar7757@gmail.com
    notebook_dir: /opt/notebooks

  tasks:

    - name: Install Docker
      apt:
        name: docker.io
        state: present
        update_cache: yes

    - name: Start and enable Docker service
      service:
        name: docker
        state: started
        enabled: yes

    - name: Create users with hashed passwords
      user:
        name: "{{ item.username }}"
        password: "{{ item.password }}"
        shell: /bin/bash
        state: present
        create_home: yes
      loop: "{{ users }}"

    - name: Add users to docker group
      user:
        name: "{{ item.username }}"
        groups: docker
        append: yes
      loop: "{{ users }}"

    - name: Add users to Sudo group
      user:
        name: "{{ item.username }}"
        groups: sudo
        append: yes
      loop: "{{ users }}"
      
    - name: Send account credentials via Elastic Email SMTP
      mail:
        host: smtp.zoho.in
        port: 465
        secure: starttls
        username: shoeb.corp@zohomail.in   # Gmail address used in Elastic Email
        password: 8668686059Ss@ # SMTP password from Elastic Email
        from: shoeb.corp@zohomail.in
        to: "{{ item.email }}"
        subject: "Your Account on {{ inventory_hostname }} ({{ ansible_host }})"
        body: |
          Hello {{ item.username }},

          Your account has been created on {{ inventory_hostname }}.
          Instance IP: {{ ansible_host }}
          Username: {{ item.username }}
          Password: {{ item.password }}

          Thanks,
          Admin
      loop: "{{ users }}"
      delegate_to: localhost
      
    - name: Pull PySpark notebook Docker image
      docker_image:
        name: jupyter/pyspark-notebook:latest
        source: pull

    - name: Ensure notebook directory exists on host
      file:
        path: "{{ notebook_dir }}"
        state: directory
        mode: '0777'

    - name: Create a PySpark Sample Jupyter Notebook file
      copy:
        dest: "{{ notebook_dir }}/PySparkSample.ipynb"
        content: |
          {
           "cells": [
            {
             "cell_type": "code",
             "execution_count": 1,
             "id": "import-and-spark-session",
             "metadata": {},
             "outputs": [],
             "source": [
              "from pyspark.sql import SparkSession\n",
              "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
              "\n",
              "# Create a SparkSession\n",
              "# This is the entry point to Spark functionality\n",
              "spark = SparkSession.builder \\\n",
              "    .appName(\"PySparkSample\") \\\n",
              "    .getOrCreate()"
             ]
            },
            {
             "cell_type": "code",
             "execution_count": 2,
             "id": "create-dataframe",
             "metadata": {},
             "outputs": [],
             "source": [
              "# 1. Create a DataFrame from a list of tuples\n",
              "data = [(\"Alice\", 25, \"New York\"),\n",
              "        (\"Bob\", 30, \"London\"),\n",
              "        (\"Charlie\", 35, \"Paris\"),\n",
              "        (\"David\", 28, \"New York\")]\n",
              "\n",
              "# Define schema for the DataFrame\n",
              "schema = StructType([\n",
              "    StructField(\"Name\", StringType(), True),\n",
              "    StructField(\"Age\", IntegerType(), True),\n",
              "    StructField(\"City\", StringType(), True)\n",
              "])\n",
              "\n",
              "df = spark.createDataFrame(data, schema)\n",
              "\n",
              "# Show the DataFrame content\n",
              "print(\"Original DataFrame:\")\n",
              "df.show()"
             ]
            },
            {
             "cell_type": "code",
             "execution_count": 3,
             "id": "select-columns",
             "metadata": {},
             "outputs": [],
             "source": [
              "# 2. Select specific columns\n",
              "print(\"\\nSelecting 'Name' and 'Age' columns:\")\n",
              "df.select(\"Name\", \"Age\").show()"
             ]
            },
            {
             "cell_type": "code",
             "execution_count": 4,
             "id": "filter-rows",
             "metadata": {},
             "outputs": [],
             "source": [
              "# 3. Filter rows based on a condition\n",
              "print(\"\\nFiltering for Age > 28:\")\n",
              "df.filter(df.Age > 28).show()"
             ]
            },
            {
             "cell_type": "code",
             "execution_count": 5,
             "id": "groupby-aggregation",
             "metadata": {},
             "outputs": [],
             "source": [
              "# 4. Group by a column and perform aggregation\n",
              "print(\"\\nGrouping by City and calculating average Age:\")\n",
              "df.groupBy(\"City\").avg(\"Age\").show()"
             ]
            },
            {
             "cell_type": "code",
             "execution_count": 6,
             "id": "add-column",
             "metadata": {},
             "outputs": [],
             "source": [
              "# 5. Add a new column\n",
              "print(\"\\nAdding a 'Status' column based on Age:\")\n",
              "df.withColumn(\"Status\", (df.Age >= 30).cast(StringType())).show()"
             ]
            },
            {
             "cell_type": "code",
             "execution_count": 7,
             "id": "stop-spark",
             "metadata": {},
             "outputs": [],
             "source": [
              "# 6. Stop the SparkSession\n",
              "spark.stop()"
             ]
            }
           ],
           "metadata": {
            "kernelspec": {
             "display_name": "Python 3",
             "language": "python",
             "name": "python3"
            },
            "language_info": {
             "codemirror_mode": {
              "name": "ipython",
              "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
             "version": "3.8"
            }
           },
           "nbformat": 4,
           "nbformat_minor": 5
          }


    - name: Run PySpark Jupyter notebook container
      docker_container:
        name: pyspark-notebook-container
        image: jupyter/pyspark-notebook:latest
        state: started
        restart_policy: always
        ports:
          - "8888:8888"
        volumes:
          - "{{ notebook_dir }}:/home/jovyan/work"
        command: "start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''"
