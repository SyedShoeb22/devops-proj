- hosts: servers
  become: yes
  collections:
    - community.docker
  vars:
    notebook_dir: /opt/pyspark-notebooks

  vars_files:
    - users.yml

  tasks:
    - name: Debug users loaded from CSV
      debug:
        var: users
  
    - name: Install Docker
      apt:
        name: docker.io
        state: present
        update_cache: yes

    - name: Start and enable Docker service
      service:
        name: docker
        state: started
        enabled: yes

    - name: Create users with hashed passwords
      user:
        name: "{{ item.username }}"
        password: "{{ item.password | password_hash('sha512') }}"
        shell: /bin/bash
        state: present
        create_home: yes
      loop: "{{ users }}"

    - name: Ensure .ssh directory exists
      file:
        path: "/home/{{ item.username }}/.ssh"
        state: directory
        owner: "{{ item.username }}"
        group: "{{ item.username }}"
        mode: '0700'
      loop: "{{ users }}"

    - name: Ensure SSH public key is installed
      authorized_key:
        user: "{{ item.username }}"
        state: present
        key: "{{ ssh_public_key }}"
      loop: "{{ users }}"
      
    - name: Add users to docker group
      user:
        name: "{{ item.username }}"
        groups: docker
        append: yes
      loop: "{{ users }}"

    - name: Add users to Sudo group
      user:
        name: "{{ item.username }}"
        groups: sudo
        append: yes
      loop: "{{ users }}"

    - name: Ensure SSH allows password authentication
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^PasswordAuthentication'
        line: 'PasswordAuthentication yes'
        state: present

    - name: Ensure KbdInteractiveAuthentication yes
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: 'KbdInteractiveAuthentication no'
        line: 'KbdInteractiveAuthentication yes'
        state: present
        
    - name: Ensure ChallengeResponseAuthentication is no
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^ChallengeResponseAuthentication'
        line: 'ChallengeResponseAuthentication no'
        state: present
    
    - name: Restart SSH daemon
      service:
        name: ssh
        state: restarted

    - name: Create notebook directory
      file:
        path: "{{ notebook_dir }}"
        state: directory
        mode: '0755'
        
    - name: Pull PySpark notebook Docker image
      docker_image:
        name: jupyter/pyspark-notebook:latest
        source: pull

    - name: Create a PySpark Sample Jupyter Notebook file
      copy:
        dest: "{{ notebook_dir }}/PySparkSample.ipynb"
        content: |
            {
             "cells": [
              {
               "cell_type": "code",
               "execution_count": null,
               "id": "hello-pyspark",
               "metadata": {},
               "outputs": [],
               "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# Create Spark session\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Hello PySpark\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "# Create a DataFrame\n",
                "data = [(\"Shoeb\", 25), (\"Abrar\", 30), (\"Sara\", 22)]\n",
                "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
                "\n",
                "# Show DataFrame\n",
                "df.show()\n",
                "\n",
                "# Stop Spark session\n",
                "spark.stop()"
               ]
              }
             ],
             "metadata": {
              "kernelspec": {
               "display_name": "Python 3 (Spark)",
               "language": "python",
               "name": "python3"
              },
              "language_info": {
               "name": "python",
               "version": ""
              }
             },
             "nbformat": 4,
             "nbformat_minor": 5
            }

    - name: Run PySpark Jupyter notebook container
      docker_container:
        name: pyspark-notebook-container
        image: jupyter/pyspark-notebook:latest
        state: started
        restart_policy: always
        ports:
          - "8888:8888"
        volumes:
          - "{{ notebook_dir }}:/home/jovyan/work"
        command: "start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''"
        
    - name: Get public IP of remote VM
      ansible.builtin.set_fact:
        vm_ip: "{{ ansible_host }}"

    # - name: Send account credentials via Elastic Email SMTP
    #   mail:
    #     host: smtp.zoho.in
    #     port: 465
    #     secure: starttls
    #     username: shoeb.corp@zohomail.in
    #     password: "{{ lookup('env', 'SMTP_ZOHO_PASS') }}"
    #     from: shoeb.corp@zohomail.in
    #     to: "{{ item.email }}"
    #     subject: "Your Account on {{ inventory_hostname }} ({{ vm_ip }})"
    #     body: |
    #       Hello {{ item.username }},

    #       Your account has been created on {{ inventory_hostname }}.
    #       Instance IP: {{ vm_ip }}
    #       Username: {{ item.username }}
    #       Password: {{ item.password }}

    #       Thanks,
    #       Admin
    #   loop: "{{ users }}"
    
    - name: Install Scala inside container
      docker_container_exec:
        container: pyspark-notebook-container
        command: |
          apt-get update && apt-get install -y wget && \
          wget https://downloads.lightbend.com/scala/2.12.15/scala-2.12.15.tgz && \
          tar xvf scala-2.12.15.tgz && \
          mv scala-2.12.15 /usr/local/scala && \
          ln -s /usr/local/scala/bin/* /usr/local/bin/ && \
          rm scala-2.12.15.tgz

    - name: Create MLlib jobs directory
      file:
        path: "{{ notebook_dir }}/jobs"
        state: directory
        mode: '0755'

    - name: Copy MLlib job scripts
      copy:
        dest: "{{ notebook_dir }}/jobs/mllib_job{{ item }}.py"
        content: |
          from pyspark.sql import SparkSession
          from pyspark.ml.classification import LogisticRegression
          from pyspark.ml.linalg import Vectors
          from pyspark.sql import Row

          spark = SparkSession.builder.appName("MLlib Job {{ item }}").master("local[1]").getOrCreate()

          data = [
              Row(label=1.0, features=Vectors.dense(0.0, 1.1, 0.1)),
              Row(label=0.0, features=Vectors.dense(2.0, 1.0, -1.0)),
              Row(label=0.0, features=Vectors.dense(2.0, 1.3, 1.0)),
              Row(label=1.0, features=Vectors.dense(0.0, 1.2, -0.5))
          ]
          df = spark.createDataFrame(data)

          lr = LogisticRegression(maxIter=10, regParam=0.01)
          model = lr.fit(df)

          print("Coefficients:", model.coefficients)
          print("Intercept:", model.intercept)

          spark.stop()
      loop: "{{ range(1, 17) | list }}"

    - name: Run 16 MLlib jobs in parallel
      shell: |
        docker exec pyspark-notebook-container \
        bash -c "for job in /home/jovyan/work/jobs/mllib_job*.py; do
          spark-submit \$job &
        done; wait"

