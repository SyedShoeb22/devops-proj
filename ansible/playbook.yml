- hosts: servers
  become: yes
  collections:
    - community.docker
  vars:
    notebook_dir: /opt/pyspark-notebooks

  vars_files:
    - users.yml

  tasks:
    - name: Debug users loaded from CSV
      debug:
        var: users
  
    - name: Install Docker
      apt:
        name: docker.io
        state: present
        update_cache: yes

    - name: Start and enable Docker service
      service:
        name: docker
        state: started
        enabled: yes

    - name: Create users with hashed passwords
      user:
        name: "{{ item.username }}"
        password: "{{ item.password | password_hash('sha512') }}"
        shell: /bin/bash
        state: present
        create_home: yes
      loop: "{{ users }}"

    - name: Ensure .ssh directory exists
      file:
        path: "/home/{{ item.username }}/.ssh"
        state: directory
        owner: "{{ item.username }}"
        group: "{{ item.username }}"
        mode: '0700'
      loop: "{{ users }}"

    - name: Ensure SSH public key is installed
      authorized_key:
        user: "{{ item.username }}"
        state: present
        key: "{{ ssh_public_key }}"
      loop: "{{ users }}"
      
    - name: Add users to docker group
      user:
        name: "{{ item.username }}"
        groups: docker
        append: yes
      loop: "{{ users }}"

    - name: Add users to Sudo group
      user:
        name: "{{ item.username }}"
        groups: sudo
        append: yes
      loop: "{{ users }}"

    - name: Ensure SSH allows password authentication
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^PasswordAuthentication'
        line: 'PasswordAuthentication yes'
        state: present
        
    - name: Get public IP of remote VM
      ansible.builtin.set_fact:
        vm_ip: "{{ ansible_host }}"

    - name: Ensure KbdInteractiveAuthentication yes
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: 'KbdInteractiveAuthentication no'
        line: 'KbdInteractiveAuthentication yes'
        state: present
        
    - name: Ensure ChallengeResponseAuthentication is no
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^ChallengeResponseAuthentication'
        line: 'ChallengeResponseAuthentication no'
        state: present
    
    - name: Restart SSH daemon
      service:
        name: ssh
        state: restarted
    - name: Create notebook directory
      file:
        path: "{{ notebook_dir }}"
        state: directory
        mode: '0755'
        
    - name: Pull PySpark notebook Docker image
      docker_image:
        name: jupyter/pyspark-notebook:latest
        source: pull

    - name: Create a PySpark Sample Jupyter Notebook file
      copy:
        dest: "{{ notebook_dir }}/PySparkSample.ipynb"
        content: |
            {
             "cells": [
              {
               "cell_type": "code",
               "execution_count": null,
               "id": "hello-pyspark",
               "metadata": {},
               "outputs": [],
               "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# Create Spark session\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Hello PySpark\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "# Create a DataFrame\n",
                "data = [(\"Shoeb\", 25), (\"Abrar\", 30), (\"Sara\", 22)]\n",
                "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
                "\n",
                "# Show DataFrame\n",
                "df.show()\n",
                "\n",
                "# Stop Spark session\n",
                "spark.stop()"
               ]
              }
             ],
             "metadata": {
              "kernelspec": {
               "display_name": "Python 3 (Spark)",
               "language": "python",
               "name": "python3"
              },
              "language_info": {
               "name": "python",
               "version": ""
              }
             },
             "nbformat": 4,
             "nbformat_minor": 5
            }

    - name: Run PySpark Jupyter notebook container
      docker_container:
        name: pyspark-notebook-container
        image: jupyter/pyspark-notebook:latest
        state: started
        restart_policy: always
        ports:
          - "8888:8888"
        volumes:
          - "{{ notebook_dir }}:/home/jovyan/work"
        command: "start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''"

    - name: Create java_mllib directory inside container
      community.docker.docker_container_exec:
        container: pyspark-notebook-container
        user: root
        command: mkdir -p /home/jovyan/work/java_mllib
        
    - name: Install wget inside container
      community.docker.docker_container_exec:
        container: pyspark-notebook-container
        user: root
        command: apt-get update && apt-get install -y wget
    
    - name: Download Scala inside container
      community.docker.docker_container_exec:
        container: pyspark-notebook-container
        user: root
        command: wget -q --show-progress --tries=5 https://downloads.lightbend.com/scala/2.12.15/scala-2.12.15.tgz -O /tmp/scala-2.12.15.tgz
    
    - name: Extract Scala inside container
      community.docker.docker_container_exec:
        container: pyspark-notebook-container
        user: root
        command: tar -xvf /tmp/scala-2.12.15.tgz -C /usr/local && mv /usr/local/scala-2.12.15 /usr/local/scala
    
    - name: Link Scala binaries
      community.docker.docker_container_exec:
        container: pyspark-notebook-container
        user: root
        command: ln -s /usr/local/scala/bin/* /usr/local/bin/

    - name: Install Scala inside container
      community.docker.docker_container_exec:
        container: pyspark-notebook-container
        user: root
        command: >
          bash -c "apt-get update &&
          apt-get install -y wget &&
          wget https://downloads.lightbend.com/scala/2.12.15/scala-2.12.15.tgz &&
          tar xvf scala-2.12.15.tgz &&
          mv scala-2.12.15 /usr/local/scala &&
          ln -s /usr/local/scala/bin/* /usr/local/bin/ &&
          rm scala-2.12.15.tgz"

    - name: Install Java in container
      community.docker.docker_container_exec:
        container: pyspark-notebook-container
        user: root
        command: bash -c "apt-get update && apt-get install -y openjdk-17-jdk"
    
    - name: Create MLlib jobs directory
      file:
        path: "{{ notebook_dir }}/jobs"
        state: directory
        mode: '0755'

    - name: Create Java MLlib job source
      copy:
        dest: "{{ notebook_dir }}/java_mllib/LogisticRegressionExample.java"
        content: |
          import org.apache.spark.sql.SparkSession;
          import org.apache.spark.ml.classification.LogisticRegression;
          import org.apache.spark.ml.linalg.Vectors;
          import org.apache.spark.sql.Row;
          import org.apache.spark.sql.Dataset;
          import org.apache.spark.sql.RowFactory;
          import org.apache.spark.sql.types.DataTypes;
          import org.apache.spark.sql.types.StructField;
          import org.apache.spark.sql.types.StructType;
          import java.util.Arrays;
          import java.util.List;
    
          public class LogisticRegressionExample {
              public static void main(String[] args) {
                  SparkSession spark = SparkSession.builder()
                          .appName("Java MLlib Logistic Regression")
                          .master("local[*]")
                          .getOrCreate();
    
                  List<Row> data = Arrays.asList(
                      RowFactory.create(1.0, Vectors.dense(0.0, 1.1, 0.1)),
                      RowFactory.create(0.0, Vectors.dense(2.0, 1.0, -1.0)),
                      RowFactory.create(0.0, Vectors.dense(2.0, 1.3, 1.0)),
                      RowFactory.create(1.0, Vectors.dense(0.0, 1.2, -0.5))
                  );
    
                  StructType schema = DataTypes.createStructType(new StructField[]{
                      DataTypes.createStructField("label", DataTypes.DoubleType, false),
                      DataTypes.createStructField("features", new org.apache.spark.ml.linalg.VectorUDT(), false)
                  });
    
                  Dataset<Row> df = spark.createDataFrame(data, schema);
    
                  LogisticRegression lr = new LogisticRegression()
                          .setMaxIter(10)
                          .setRegParam(0.01);
    
                  lr.fit(df).summary().predictions().show();
    
                  spark.stop();
              }
          }

    - name: Compile Java MLlib job
      community.docker.docker_container_exec:
        container: pyspark-notebook-container
        user: root
        command: bash -c "javac -cp '/usr/local/spark/jars/*' /home/jovyan/work/java_mllib/LogisticRegressionExample.java"
    
    - name: Run Java MLlib job
      community.docker.docker_container_exec:
        container: pyspark-notebook-container
        command: bash -c "spark-submit --class LogisticRegressionExample --master local[*] /home/jovyan/work/java_mllib/LogisticRegressionExample.class"

    - name: Run 16 Java MLlib jobs in parallel
      community.docker.docker_container_exec:
        container: pyspark-notebook-container
        command: bash -c "for job in /home/jovyan/work/java_mllib/*.class; do spark-submit --class $(basename $job .class) --master local[*] $job & done; wait"
  



    # - name: Send account credentials via Elastic Email SMTP
    #   mail:
    #     host: smtp.zoho.in
    #     port: 465
    #     secure: starttls
    #     username: shoeb.corp@zohomail.in
    #     password: "{{ lookup('env', 'SMTP_ZOHO_PASS') }}"
    #     from: shoeb.corp@zohomail.in
    #     to: "{{ item.email }}"
    #     subject: "Your Account on {{ inventory_hostname }} ({{ vm_ip }})"
    #     body: |
    #       Hello {{ item.username }},

    #       Your account has been created on {{ inventory_hostname }}.
    #       Instance IP: {{ vm_ip }}
    #       Username: {{ item.username }}
    #       Password: {{ item.password }}

    #       Thanks,
    #       Admin
    #   loop: "{{ users }}"
    


