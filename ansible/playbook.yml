---
- name: Setup Spark All-in-One Docker Image and Push to Docker Hub
  hosts: all
  become: yes
  vars:
    dockerhub_username: "YOUR_DOCKERHUB_USERNAME"
    dockerhub_password: "YOUR_DOCKERHUB_PASSWORD"
    image_name: "spark-all-in-one"
    image_tag: "latest"
    spark_version: "3.5.1"
    scala_version: "2.12.15"
    hadoop_version: "3"
    python_version: "3.10"
    users_list:
      - arun
      - akash
      - abdul
      - indresh
      - farhaan
      - manoj
      - yashwanth
      - immanuel
      - abhishek
      - nalle
      - aluru
      - nagakiran
      - nagaraju
      - alby
      - anas
      - mujahed

  tasks:
    - name: Install required packages
      apt:
        name:
          - docker.io
          - git
          - python3-pip
        state: present
        update_cache: yes

    - name: Install Docker Compose
      pip:
        name: docker-compose

    - name: Ensure Docker service is running
      service:
        name: docker
        state: started
        enabled: yes

    - name: Create project directory
      file:
        path: /opt/spark-all-in-one
        state: directory

    - name: Create Dockerfile
      copy:
        dest: /opt/spark-all-in-one/Dockerfile
        content: |
          FROM openjdk:17-jdk-slim

          LABEL maintainer="Mujahed Sir NubeEra"
          LABEL description="Spark + Scala + PySpark + SQL + Java + MLlib All-in-One"

          ENV SCALA_VERSION={{ scala_version }} \
              SPARK_VERSION={{ spark_version }} \
              HADOOP_VERSION={{ hadoop_version }} \
              PYTHON_VERSION={{ python_version }}

          RUN apt-get update && apt-get install -y \
              curl wget python3 python3-pip sudo ssh \
              && rm -rf /var/lib/apt/lists/*

          # Install Scala
          RUN wget https://downloads.lightbend.com/scala/${SCALA_VERSION}/scala-${SCALA_VERSION}.tgz && \
              tar xvf scala-${SCALA_VERSION}.tgz && \
              mv scala-${SCALA_VERSION} /usr/local/scala && \
              ln -s /usr/local/scala/bin/* /usr/local/bin/ && \
              rm scala-${SCALA_VERSION}.tgz

          # Install Spark
          RUN wget https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
              tar xvf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
              mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /usr/local/spark && \
              ln -s /usr/local/spark/bin/* /usr/local/bin/ && \
              rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

          # Install PySpark & MLlib dependencies
          RUN pip3 install pyspark jupyterlab numpy pandas matplotlib

          # Create specific users
          RUN for u in {{ users_list | join(' ') }}; do \
                useradd -m -s /bin/bash $u && \
                echo "$u:$u" | chpasswd && \
                adduser $u sudo; \
              done

          ENV PATH=$PATH:/usr/local/spark/bin:/usr/local/scala/bin \
              SPARK_HOME=/usr/local/spark \
              SCALA_HOME=/usr/local/scala \
              PYSPARK_PYTHON=python3

          EXPOSE 8888 4040 8080 7077

          CMD ["bash", "-c", "start-notebook.sh --NotebookApp.token='' --NotebookApp.ip=0.0.0.0 --NotebookApp.allow_root=True"]

    - name: Build Docker image
      community.docker.docker_image:
        build:
          path: /opt/spark-all-in-one
        name: "{{ dockerhub_username }}/{{ image_name }}"
        tag: "{{ image_tag }}"
        push: no

    - name: Login to Docker Hub
      community.docker.docker_login:
        username: "{{ dockerhub_username }}"
        password: "{{ dockerhub_password }}"

    - name: Push image to Docker Hub
      community.docker.docker_image:
        name: "{{ dockerhub_username }}/{{ image_name }}"
        tag: "{{ image_tag }}"
        push: yes
